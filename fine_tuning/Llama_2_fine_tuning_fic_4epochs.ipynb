{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1860/LDS_Project/blob/main/fine_tuning/Llama_2_fine_tuning_fic_4epochs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WCo6_AdU8hpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 tensorboard huggingface_hub[cli] xformers"
      ],
      "metadata": {
        "id": "tKcD8rnA8jm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "2jz0TTbO7z8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cx-fTS368eS2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    )\n",
        "from datasets import load_dataset, Dataset\n",
        "from trl import SFTTrainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Linguistic Data Science/data/bnc_baby_texts.csv', sep='|')\n",
        "df"
      ],
      "metadata": {
        "id": "D7tJ8xbBSRGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aca, df_dem, df_fic, df_news = [y for x, y in df.groupby(['category'])]"
      ],
      "metadata": {
        "id": "t1k2SDf7UCce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_aca"
      ],
      "metadata": {
        "id": "Z9WNQA0nVIME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aca_dataset = Dataset.from_pandas(df_aca)\n",
        "fic_dataset = Dataset.from_pandas(df_fic)\n",
        "news_dataset = Dataset.from_pandas(df_news)"
      ],
      "metadata": {
        "id": "NEtoC585xw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aca_dataset"
      ],
      "metadata": {
        "id": "o7oiSpSpqJfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Llama 2\n",
        "\n",
        "Norouzi (2023) describes an efficient way to handle Llama 2 despite the system RAM limitations that Google Colab comes with and was, therefore, used as the main source throughout this section. For more details see: [Mastering Llama 2: A Comprehensive Guide to Fine-Tuning in Google Colab](https://medium.com/artificial-corner/mastering-llama-2-a-comprehensive-guide-to-fine-tuning-in-google-colab-bedfcc692b7f). Additionally, [this notebook](https://colab.research.google.com/drive/12dVqXZMIVxGI0uutU6HG9RWbWPXL3vts?authuser=2#scrollTo=qmA4G6C64dJ4) was used as a reference for further parameters."
      ],
      "metadata": {
        "id": "1TZl-H3Y3nYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaring the model name; as described in the paper, the 7 billion parameter version of Llama 2 is used\n",
        "model_name = \"meta-llama/Llama-2-7b-hf\""
      ],
      "metadata": {
        "id": "BtxN9t6T5SWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for the quantization config need to be defined:"
      ],
      "metadata": {
        "id": "tSMzUq4_65kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the entire model on the GPU 0\n",
        "device_map = {\"\": 0}\n",
        "\n",
        "# Set base model loading in 4-bits\n",
        "use_4bit = True\n",
        "\n",
        "# Compute dtype for 4-bit base models\n",
        "bnb_4bit_compute_dtype = torch.float16\n",
        "\n",
        "# Quantization type (fp4 or nf4); nf4 is shown to be better in the QLoRA paper\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "_sbIxyo60eWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the quantization config using the previously declared parameters\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant\n",
        ")"
      ],
      "metadata": {
        "id": "v1uQbzEk7JcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we got both the `device_map` set to one GPU and the quantization config `bnb_config` initialized, the base model can be loaded into the system without any memory issues:"
      ],
      "metadata": {
        "id": "NGbXcggJ7ptF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "7AeGyB1A7m8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the corresponding tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "BNkbinkFx0lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the original tokenizer has `pad_id` set to -1, we need to to define a custom padding token. While the [Llama 2 huggingface documentation](https://huggingface.co/docs/transformers/main/model_doc/llama2#overview) recommends using `tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})`, Norouzi points out that this can introduce CUDA-related errors. Therefore, he sees directly setting the `pad_token` as a safer option.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60P9cm5705i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a custom padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Set the padding direction to the right\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "8CUqUJtR0ghR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters are set in accordance to the QLoRA method."
      ],
      "metadata": {
        "id": "9Kjp4GQLBSFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA attention dimension\n",
        "lora_r = 64\n",
        "# Alpha for LoRA scaling\n",
        "lora_alpha = 16\n",
        "# Dropout probability for LoRA\n",
        "lora_dropout = 0.1"
      ],
      "metadata": {
        "id": "iL32tqSiAjSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "f3wZBUjwBUXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"/content/drive/MyDrive/results/fictional\"\n",
        "final_checkpoint_dir = os.path.join(output_dir, \"2e_final_checkpoint\")"
      ],
      "metadata": {
        "id": "3SN8UCHwtgIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While most hyperparameters remain the same throughout the different variants, the number of epochs will be varied from 2 to 4 and 8."
      ],
      "metadata": {
        "id": "LlcWk82jZPT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "per_device_train_batch_size = 1\n",
        "gradient_accumulation_steps = 1\n",
        "num_train_epochs = 4, #2, 8\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 10\n",
        "logging_steps = 5\n",
        "learning_rate = 4e-5\n",
        "max_grad_norm = 0.3\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\""
      ],
      "metadata": {
        "id": "kpF4Nw4Av0nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    num_train_epochs=num_train_epochs\n",
        ")"
      ],
      "metadata": {
        "id": "U47U6tQov6pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2500\n",
        "packing = False"
      ],
      "metadata": {
        "id": "H1zcf1gQv8eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fic_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=fic_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments\n",
        ")"
      ],
      "metadata": {
        "id": "2zICfwc6wCBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_checkpoint = False"
      ],
      "metadata": {
        "id": "TwTDSnJ92HEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.logging.set_verbosity_info()"
      ],
      "metadata": {
        "id": "nQRd4TmS2JFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in fic_trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)"
      ],
      "metadata": {
        "id": "SLakdti5PWyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fic_trainer.train(resume_checkpoint)"
      ],
      "metadata": {
        "id": "Aj-ZYmhi2KdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving and uploading the model:"
      ],
      "metadata": {
        "id": "ZPN9oNXg_GQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = fic_trainer.model.module if hasattr(fic_trainer.model, 'module') else fic_trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ],
      "metadata": {
        "id": "TXga358N3Ogf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig.from_pretrained('outputs')\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "xH3x6klmPcNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(f\"dbauer1860/llama-2-bnc-baby-fictional-{num_train_epochs}-epochs\", create_pr=1)"
      ],
      "metadata": {
        "id": "PZDRiWRcAL6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the relevant log information to plot in another notebook"
      ],
      "metadata": {
        "id": "LoDyHyhL_NBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fic_trainer.state.log_history"
      ],
      "metadata": {
        "id": "aGJxo-57B0Qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_values = []\n",
        "epochs = []\n",
        "\n",
        "for step in fic_trainer.state.log_history:\n",
        "  for k, v in step.items():\n",
        "    if k == 'loss' and v not in loss_values:\n",
        "      loss_values.append(v)\n",
        "    if k == 'epoch'and v not in epochs:\n",
        "      epochs.append(v)\n",
        "\n",
        "print(epochs)"
      ],
      "metadata": {
        "id": "Ds8bKaX0-gmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'loss': loss_values, 'epoch': epochs})\n",
        "df"
      ],
      "metadata": {
        "id": "kiwqy6mKCioH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Linguistic Data Science/data/logs/fictional_4e_logs.csv\", sep=\";\", index=False)"
      ],
      "metadata": {
        "id": "ljHjgHgsDzTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}